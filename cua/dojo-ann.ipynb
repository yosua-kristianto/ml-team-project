{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow;\n",
    "import pandas;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from tensorflow import keras;\n",
    "from tensorflow.keras.models import Sequential;\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt;\n",
    "from tensorflow.keras.utils import plot_model;\n",
    "import requests;\n",
    "import time;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0      20000    2          2         1   24      2      2     -1     -1   \n",
       "1     120000    2          2         2   26     -1      2      0      0   \n",
       "2      90000    2          2         2   34      0      0      0      0   \n",
       "3      50000    2          2         1   37      0      0      0      0   \n",
       "4      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0     -2  ...          0          0          0         0       689         0   \n",
       "1      0  ...       3272       3455       3261         0      1000      1000   \n",
       "2      0  ...      14331      14948      15549      1518      1500      1000   \n",
       "3      0  ...      28314      28959      29547      2000      2019      1200   \n",
       "4      0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  LABEL  \n",
       "0         0         0         0      1  \n",
       "1      1000         0      2000      1  \n",
       "2      1000      1000      5000      0  \n",
       "3      1100      1069      1000      0  \n",
       "4      9000       689       679      0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data from pre-cleaned csv file\n",
    "current_folder = \"../\";\n",
    "# current_folder = \"\";\n",
    "dataframe = pandas.read_csv(current_folder + \"credit_card_clients.csv\");\n",
    "\n",
    "dataframe = dataframe.drop(columns = [\"ID\"]);\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance based on LightGBM\n",
    "\n",
    "This time, we gotta use the top 10 the most important features according to LightGBM\n",
    "\n",
    "PAY_0\n",
    "PAY_2\n",
    "PAY_AMT2\n",
    "LIMIT_BAL\n",
    "PAY_AMT3\n",
    "BILL_AMT1\n",
    "PAY_3\n",
    "PAY_4\n",
    "PAY_6\n",
    "PAY_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping everything that other than mentioned above\n",
      "Dropping feature SEX since its not \"important\"\n",
      "Dropping feature EDUCATION since its not \"important\"\n",
      "Dropping feature MARRIAGE since its not \"important\"\n",
      "Dropping feature AGE since its not \"important\"\n",
      "Dropping feature BILL_AMT2 since its not \"important\"\n",
      "Dropping feature BILL_AMT3 since its not \"important\"\n",
      "Dropping feature BILL_AMT4 since its not \"important\"\n",
      "Dropping feature BILL_AMT5 since its not \"important\"\n",
      "Dropping feature BILL_AMT6 since its not \"important\"\n",
      "Dropping feature PAY_AMT1 since its not \"important\"\n",
      "Dropping feature PAY_AMT4 since its not \"important\"\n",
      "Dropping feature PAY_AMT5 since its not \"important\"\n",
      "Dropping feature PAY_AMT6 since its not \"important\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Dropping everything that other than mentioned above\");\n",
    "\n",
    "most_important_features = [\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_AMT2\",\n",
    "    \"LIMIT_BAL\",\n",
    "    \"PAY_AMT3\",\n",
    "    \"BILL_AMT1\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_6\",\n",
    "    \"PAY_5\",\n",
    "    \"LABEL\" # Don't :)\n",
    "];\n",
    "\n",
    "columns_to_be_dropped = [];\n",
    "for i in dataframe.columns:\n",
    "    if i not in most_important_features:\n",
    "        columns_to_be_dropped.append(i);\n",
    "        print(f\"Dropping feature {i} since its not \\\"important\\\"\");\n",
    "\n",
    "dataframe = dataframe.drop(columns = columns_to_be_dropped);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "Checkout the `main.ipynb` since I just copy-pasting the whole thing from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after cleaning:  (30000, 11)\n",
      "['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'LABEL']\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after cleaning: \", dataframe.shape);\n",
    "print(dataframe.columns.tolist());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape:  (30000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define label data\n",
    "label = dataframe['LABEL'];\n",
    "\n",
    "# Drop ID, SEX, EDUCATION, MARRIAGE, and LABEL from dataframe for features\n",
    "features = dataframe.drop(columns=['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', \"LABEL\"]);\n",
    "\n",
    "# Typecasting all values within to int64 because ANN need to be standardized\n",
    "\n",
    "for i in dataframe.columns:\n",
    "    dataframe[i] = dataframe[i].astype(int);\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(features, label, train_size = 0.7, test_size = 0.3, random_state = 42);\n",
    "feature_validation, feature_test, label_validation, label_test = train_test_split(feature_test, label_test, test_size=0.5, random_state = 42);\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler();\n",
    "feature_train_scaled = scaler.fit_transform(feature_train);\n",
    "feature_validation_scaled = scaler.fit_transform(feature_validation);\n",
    "feature_test_scaled = scaler.transform(feature_test);\n",
    "\n",
    "print(\"Features shape: \", features.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telegram_reporter(message):\n",
    "    message = \"[\"+ time.strftime(\"%Y-%m-%d %H:%M:%S\") +\"] \" + message\n",
    "    requests.request(method=\"POST\", url=\"https://api.telegram.org/bot:telegram_bot_token/sendMessage?chat_id=:your_chat_id&text=\" + message, headers={}, data={});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tunning\n",
    "\n",
    "This code based on Tensorflow Artifical Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "(None, 512)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 128)\n",
      "(None, 128)\n",
      "(None, 128)\n",
      "(None, 64)\n",
      "(None, 64)\n",
      "(None, 64)\n",
      "(None, 64)\n",
      "(None, 64)\n",
      "(None, 64)\n",
      "(None, 64)\n",
      "(None, 64)\n",
      "(None, 32)\n",
      "(None, 32)\n",
      "(None, 32)\n",
      "(None, 32)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 512)               6144      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 255937 (999.75 KB)\n",
      "Trainable params: 253313 (989.50 KB)\n",
      "Non-trainable params: 2624 (10.25 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 11), found shape=(None, 4)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model;\n\u001b[1;32m    126\u001b[0m dojo \u001b[38;5;241m=\u001b[39m ANNModel();\n\u001b[0;32m--> 127\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdojo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m    129\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(feature_test_scaled, label_test);\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy);\n",
      "Cell \u001b[0;32mIn[19], line 108\u001b[0m, in \u001b[0;36mANNModel.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m telegram_reporter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training model with code [202401291503] with total epoch of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch()));\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Fitting\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m model_plot \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_validation_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_validation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback_model_backup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback_early_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m    116\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(model_plot\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Training Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n\u001b[1;32m    117\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(model_plot\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Validation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6b13mf8b.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/cua144/anaconda3/envs/ml_team_project_ann/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 11), found shape=(None, 4)\n"
     ]
    }
   ],
   "source": [
    "class ANNModel:\n",
    "\n",
    "    # Hyperparameters\n",
    "    def input_layer(self):\n",
    "        return keras.layers.InputLayer(input_shape = (11,)); # 11 is total features dimension\n",
    "\n",
    "    def hidden_layer(self):\n",
    "        return [\n",
    "            keras.layers.Dense(512, kernel_initializer = \"glorot_uniform\", kernel_regularizer = tensorflow.keras.regularizers.l2(1e-4), activity_regularizer = tensorflow.keras.regularizers.l2(1e-4)),\n",
    "            keras.layers.Activation(\"sigmoid\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.8),\n",
    "\n",
    "            keras.layers.Dense(256, kernel_initializer = \"glorot_uniform\", kernel_regularizer = tensorflow.keras.regularizers.l2(1e-4), activity_regularizer = tensorflow.keras.regularizers.l2(1e-4)),\n",
    "            keras.layers.Activation(\"sigmoid\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "\n",
    "            keras.layers.Dense(256, kernel_initializer = \"glorot_uniform\", kernel_regularizer = tensorflow.keras.regularizers.l2(1e-4), activity_regularizer = tensorflow.keras.regularizers.l2(1e-4)),\n",
    "            keras.layers.Activation(\"sigmoid\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "\n",
    "            keras.layers.Dense(128, kernel_initializer = \"glorot_uniform\", kernel_regularizer = tensorflow.keras.regularizers.l2(1e-4), activity_regularizer = tensorflow.keras.regularizers.l2(1e-4)),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "        ];\n",
    "\n",
    "    def shallow_hidden_layer(self):\n",
    "        return [\n",
    "            keras.layers.Dense(64, kernel_initializer = \"glorot_uniform\", kernel_regularizer = tensorflow.keras.regularizers.l2(1e-4)),\n",
    "            keras.layers.Activation(\"sigmoid\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "\n",
    "            keras.layers.Dense(64, kernel_initializer = \"glorot_uniform\", kernel_regularizer = tensorflow.keras.regularizers.l2(1e-4)),\n",
    "            keras.layers.Activation(\"sigmoid\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "\n",
    "            keras.layers.Dense(32, kernel_initializer = \"glorot_uniform\", kernel_regularizer = tensorflow.keras.regularizers.l2(1e-3)),\n",
    "            keras.layers.Activation(\"sigmoid\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "        ];\n",
    "\n",
    "    def output_layer(self):\n",
    "        return keras.layers.Dense(1, kernel_initializer = \"glorot_uniform\", activation = \"sigmoid\"); # Since output is only 1 label.\n",
    "\n",
    "    def optimizer(self):\n",
    "        return keras.optimizers.Adam(learning_rate = 1e-3);\n",
    "\n",
    "    def loss(self):\n",
    "        return \"binary_crossentropy\";\n",
    "\n",
    "    def metrics(self):\n",
    "        return \"accuracy\";\n",
    "\n",
    "    def epoch(self):\n",
    "        return 1000;\n",
    "\n",
    "        \n",
    "    # Callbacks\n",
    "    # Backup Model to h5 file\n",
    "    def _callback_model_backup(self):\n",
    "        checkpoint_path = current_folder + \"/cua/\" + \"model_checkpoint.h5\";\n",
    "\n",
    "        return tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            save_weights_only=True,\n",
    "            save_freq='epoch'\n",
    "        );\n",
    "\n",
    "    # Early stopping after loss are not improved for some epochs\n",
    "    def _callback_early_stopping(self):\n",
    "        early_stopping_tolerance = 50;\n",
    "\n",
    "        return tensorflow.keras.callbacks.EarlyStopping(\n",
    "            monitor = \"val_loss\",\n",
    "            patience = early_stopping_tolerance,\n",
    "            restore_best_weights = True\n",
    "        );\n",
    "\n",
    "    # Fitting\n",
    "    def fit(self):\n",
    "\n",
    "        # Construct the model \n",
    "        model = Sequential();\n",
    "        model.add(self.input_layer());\n",
    "\n",
    "        for i in [self.hidden_layer(), self.shallow_hidden_layer()]:\n",
    "            for j in i:\n",
    "                model.add(j);\n",
    "                print(model.output_shape)\n",
    "\n",
    "        model.add(self.output_layer());\n",
    "\n",
    "        # Draw the model architecture\n",
    "        plot_model(model, to_file=(current_folder + \"/cua/\" + \"model_architecture.png\"), show_shapes=True)\n",
    "        model.summary()\n",
    "\n",
    "        # Compile the model pipeline\n",
    "        model.compile(loss = self.loss(), optimizer = self.optimizer(), metrics=[self.metrics()]);\n",
    "\n",
    "        telegram_reporter(\"Starting training model with code [202401291503] with total epoch of \" + str(self.epoch()));\n",
    "\n",
    "        # Fitting\n",
    "        model_plot = model.fit(\n",
    "            feature_train_scaled, \n",
    "            label_train, \n",
    "            epochs = self.epoch(), \n",
    "            validation_data = (feature_validation_scaled, label_validation), \n",
    "            callbacks = [self._callback_model_backup(), self._callback_early_stopping()] \n",
    "        );\n",
    "\n",
    "        plt.plot(model_plot.history[\"loss\"], label = \"Model Training Loss\");\n",
    "        plt.plot(model_plot.history[\"val_loss\"], label = \"Model Validation Loss\");\n",
    "        plt.title(\"Training and Validation Loss\");\n",
    "        plt.xlabel(\"Epochs\");\n",
    "        plt.ylabel(\"Loss\");\n",
    "        plt.legend();\n",
    "        plt.show();\n",
    "\n",
    "        return model;\n",
    "\n",
    "dojo = ANNModel();\n",
    "model = dojo.fit();\n",
    "\n",
    "loss, accuracy = model.evaluate(feature_test_scaled, label_test);\n",
    "print(\"Loss: \", loss, \" Accuracy: \", accuracy);\n",
    "\n",
    "telegram_reporter(\"Finished training with Loss: \" + str(loss) + \" Accuracy: \" + str(accuracy));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
